{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb042e1f",
   "metadata": {},
   "source": [
    "# Utterance generation prototype notebook\n",
    "Author: Matthew Stachyra <br>\n",
    "Date: 16 June 2022 <br>\n",
    "Version: 0.1 - prototyping 3 approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773341b8",
   "metadata": {},
   "source": [
    "## *Approach 1:* replacement with similar words\n",
    "Note: This can be used to generate very similar sentences with similar structure. They may also be used for the machine learning in approaches 2 and 3 below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb5f34",
   "metadata": {},
   "source": [
    "### Subproblems\n",
    "1. generate possible synonyms\n",
    "2. filter synonyms using similarity measure\n",
    "2. identify which words to replace in an utterance\n",
    "3. replace words in utterance one at a time and generate new set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "25d7d3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/matthewstachyra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/matthewstachyra/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59cde8",
   "metadata": {},
   "source": [
    "### 1. generate similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204050f",
   "metadata": {},
   "source": [
    "##### using `nltk.corpus.wordnet.synsets` and `spacy` part of speech tagging\n",
    "nltk doc: https://www.nltk.org/howto/wordnet.html <br>\n",
    "spacy doc: https://spacy.io/usage/linguistic-features#pos-tagging <br>\n",
    "pos tags used in spacy: https://universaldependencies.org/u/pos/ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "063d40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "posmap = {'VERB':'v', 'NOUN':'n', 'PRON':'n', 'PROPN':'n', 'ADJ':'a', 'ADV':'r'}\n",
    "\n",
    "def preprocess(utterance):\n",
    "    '''return list of words in utterance preprocessed to be lower case, removing any non \n",
    "    alphabetic characters, removing words less than 2 characters, removing \n",
    "    '''\n",
    "    utterance = utterance.lower()\n",
    "    cleanr    = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', utterance)\n",
    "    rem_num   = re.sub('[0-9]+', '', cleantext)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    return \" \".join(tokenizer.tokenize(rem_num))\n",
    "\n",
    "\n",
    "def get_pos(word, utterance):\n",
    "    '''return the part of speech of the word in the utterance if it is a verb\n",
    "    noun, pronoun, proper noun, adjective, or adverb.\n",
    "    '''\n",
    "    if not utterance or not word: raise ValueError(\"Error: Input is empty string.\")\n",
    "    if word not in utterance:     raise ValueError(\"Error: The word is not in the utterance.\")\n",
    "        \n",
    "    for w in nlp(utterance):\n",
    "        if str(w)==word: return w.pos_\n",
    "        \n",
    "\n",
    "def get_synonyms(word, utterance):\n",
    "    '''return synonyms by taking the lemma generated by synsets that have the same part of speech,\n",
    "    given a word if its part of speech is a verb, noun, adverb, or adjective.\n",
    "    \n",
    "    Note: it is necessary to pass in the pos to get the relevant kind of synonym.\n",
    "    '''\n",
    "    pos = get_pos(word, utterance)\n",
    "    \n",
    "    if pos not in ['VERB', 'NOUN', 'PRON', 'PROPN', 'ADJ', 'ADV']: return \n",
    "    \n",
    "    return set(list(itertools.chain(\n",
    "        [synonym\n",
    "         for synset in wn.synsets(word, pos=posmap[pos])\n",
    "         for synonym in synset.lemma_names()\n",
    "         if len(word)>1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8dc3f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "sometext = \"Will I need to go to the doctor again next Monday?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "2e95fbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will i need to go to the doctor again next monday\n"
     ]
    }
   ],
   "source": [
    "clean = preprocess(sometext)\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a72f317d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ask',\n",
       " 'call_for',\n",
       " 'demand',\n",
       " 'involve',\n",
       " 'necessitate',\n",
       " 'need',\n",
       " 'postulate',\n",
       " 'require',\n",
       " 'take',\n",
       " 'want'}"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_synonyms('need', clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7100a3",
   "metadata": {},
   "source": [
    "### 2. filter synonyms using similarity measure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc902249",
   "metadata": {},
   "source": [
    "##### using `gensim` with `GloVe` embeddings\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f384faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with two models to see which performs best\n",
    "model_glove_twitter = api.load(\"glove-twitter-25\")\n",
    "model_gigaword = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c54b4d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('should', 0.8821427226066589),\n",
       " ('want', 0.8693705797195435),\n",
       " ('we', 0.8659201264381409),\n",
       " ('must', 0.8644395470619202),\n",
       " ('needed', 0.8635740280151367),\n",
       " ('needs', 0.8618939518928528),\n",
       " ('get', 0.8493343591690063),\n",
       " ('make', 0.8488180637359619),\n",
       " ('do', 0.8422726988792419),\n",
       " ('able', 0.8364372849464417)]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gigaword.most_similar(positive=['need'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5d5c9fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('take', 0.9688727259635925),\n",
       " ('get', 0.9679074883460999),\n",
       " ('give', 0.9652544856071472),\n",
       " ('make', 0.9647879004478455),\n",
       " (\"n't\", 0.9613597393035889),\n",
       " ('better', 0.9595354199409485),\n",
       " (\"'ll\", 0.9594433903694153),\n",
       " ('let', 0.9594159126281738),\n",
       " ('bring', 0.9558347463607788),\n",
       " ('have', 0.9553070068359375)]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.most_similar(positive=['need'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9523a0",
   "metadata": {},
   "source": [
    "**Note:** it seems that the wikipedia model provides more relevant 'synonyms' than the model trained on twitter. The below code works exclusively with `model_gigaword`, or the embeddings trained on wikipedia data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "4218526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(vector, model):\n",
    "    '''return a (100,) embedding of the vector using the model.\n",
    "    '''\n",
    "    try:\n",
    "        vec = model.get_vector(vector)\n",
    "    except:\n",
    "        return np.empty(0)\n",
    "    \n",
    "    return vec\n",
    "\n",
    "    \n",
    "def get_similarities(word, synonyms, model):\n",
    "    '''return dictionary with synonym:cosine similarity key-value pairs.\n",
    "    '''\n",
    "    cosinesim = lambda v1, v2: (np.dot(v1, v2 / (norm(v1) * norm(v2))))\n",
    "    \n",
    "    sims = {word: 1.0}\n",
    "    ref  = embed(word, model)\n",
    "    \n",
    "    for s in synonyms:\n",
    "        vec = embed(s, model)                 \n",
    "        if vec.any():\n",
    "            sim     = cosinesim(ref, vec)\n",
    "            sims[s] = sim\n",
    "    \n",
    "    return sims\n",
    "\n",
    "        \n",
    "def filter_synonyms(similarities, threshold=0.70):\n",
    "    '''return subset of dictionary where the similarity is at least the threshold\n",
    "    value, with a default of 0.70 cosine similarity.\n",
    "    '''\n",
    "    return [synonym \n",
    "            for synonym, similarity in similarities.items() \n",
    "            if similarity>=threshold]\n",
    "    \n",
    "\n",
    "def print_similarities(similarities):\n",
    "    '''print each word with its cosine similarity to a reference vector.\n",
    "    '''\n",
    "    for synonym, similarity in similarities.items():\n",
    "        print(f\"word: {synonym}, cosine similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "1e632f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.043244, -0.47529 ,  0.15808 ,  0.20413 , -0.15383 ,  0.72284 ,\n",
       "        0.26145 ,  0.20892 , -0.3147  , -0.070307, -0.43367 ,  0.053109,\n",
       "        0.73635 ,  0.98111 ,  0.23535 , -0.10449 ,  0.50258 , -0.033356,\n",
       "       -0.35537 ,  0.64549 , -0.37103 , -0.10052 , -0.76929 , -0.16957 ,\n",
       "       -0.15648 ,  0.53548 ,  0.35146 , -1.5126  ,  0.050984,  0.24445 ,\n",
       "       -0.35688 ,  0.43968 , -0.62985 ,  0.32891 , -0.53009 ,  0.49832 ,\n",
       "       -1.2061  ,  0.27797 ,  0.42734 ,  0.095773, -0.43527 ,  0.93561 ,\n",
       "        0.36039 , -0.83114 ,  0.12966 , -0.1363  , -0.58124 ,  0.092946,\n",
       "       -0.014708,  0.32562 ,  0.41204 ,  0.1451  ,  0.49803 ,  0.86926 ,\n",
       "       -0.18033 , -1.6227  , -0.64565 ,  0.17504 ,  0.73849 ,  0.39156 ,\n",
       "        0.83135 ,  0.51308 ,  0.12999 , -0.21288 ,  0.68456 ,  0.056297,\n",
       "        0.090792,  0.28032 , -0.12233 ,  0.60761 , -0.57913 , -0.024127,\n",
       "       -0.063252,  0.40747 ,  0.10775 ,  0.57977 ,  0.092789, -0.15588 ,\n",
       "       -0.36494 , -0.46632 ,  0.37553 ,  0.164   , -0.75138 , -0.10833 ,\n",
       "       -2.1363  , -0.11658 ,  0.23157 ,  0.24425 , -0.36352 , -0.34928 ,\n",
       "        0.60001 , -0.10529 ,  0.78614 ,  0.45707 , -0.1712  ,  0.6362  ,\n",
       "        0.68622 , -0.28397 , -0.29773 , -0.81148 ], dtype=float32)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gigaword.get_vector('doctor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "2a85d9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: need, cosine similarity: 1.0\n",
      "word: ask, cosine similarity: 0.7399007678031921\n",
      "word: necessitate, cosine similarity: 0.18295712769031525\n",
      "word: demand, cosine similarity: 0.6003023386001587\n",
      "word: postulate, cosine similarity: 0.018827084451913834\n",
      "word: involve, cosine similarity: 0.5790201425552368\n",
      "word: take, cosine similarity: 0.8283353447914124\n",
      "word: want, cosine similarity: 0.8693705797195435\n",
      "word: require, cosine similarity: 0.7587778568267822\n"
     ]
    }
   ],
   "source": [
    "word = 'need' \n",
    "sims = get_similarities(word,\n",
    "                        get_synonyms(word, clean),\n",
    "                        model_gigaword)\n",
    "print_similarities(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "faac6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context for the word is: \n",
      " 'will i need to go to the doctor again next monday'. \n",
      "\n",
      "Full list of synonyms is: \n",
      " ['ask', 'necessitate', 'need', 'call_for', 'demand', 'postulate', 'involve', 'take', 'want', 'require']. \n",
      "\n",
      "Filtered list of synonyms is: \n",
      " ['need', 'ask', 'take', 'want', 'require'].\n"
     ]
    }
   ],
   "source": [
    "print(f\"The context for the word is: \\n '{clean}'. \\n\")\n",
    "print(f\"Full list of synonyms is: \\n {list(get_synonyms(word, clean))}. \\n\")\n",
    "print(f\"Filtered list of synonyms is: \\n {filter_synonyms(sims)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec420d71",
   "metadata": {},
   "source": [
    "### 3. select which words to replace \n",
    "Note: only replacing nouns, verbs, pronouns, proper nouns, adjectives, and adverbs <br>\n",
    "spacy doc: https://spacy.io/usage/linguistic-features#pos-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6742463b",
   "metadata": {},
   "source": [
    "##### using `spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "b6e6158a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUX\n",
      "PRON\n",
      "VERB\n",
      "PART\n",
      "VERB\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "ADV\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n"
     ]
    }
   ],
   "source": [
    "for word in nlp(sometext):\n",
    "    print(word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "49283613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pos('need', preprocess(sometext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d28f3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonyms_map(utterance, synonymfilter=False):\n",
    "    '''return dictionary of words to synonyms for words, removing any words \n",
    "    that do not have synonyms returned.\n",
    "    \n",
    "    NOTE  current version removes ngrams.\n",
    "    '''\n",
    "    d = {}\n",
    "    \n",
    "    for word in preprocess(utterance).split():\n",
    "        synonyms = get_synonyms(word, clean)\n",
    "        if synonyms:\n",
    "            similarities = get_similarities(word, synonyms, model_gigaword)\n",
    "            synonyms     = filter_synonyms(similarities) if synonymfilter else synonyms\n",
    "            synonyms     = [synonym\n",
    "                            for synonym in synonyms \n",
    "                            if len(synonym.split(\"_\"))==1 and preprocess(synonym)!=word] \n",
    "        \n",
    "        if synonyms: d[word] = synonyms \n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "0317fd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'need': ['ask',\n",
       "  'necessitate',\n",
       "  'demand',\n",
       "  'postulate',\n",
       "  'involve',\n",
       "  'take',\n",
       "  'want',\n",
       "  'require'],\n",
       " 'go': ['survive',\n",
       "  'depart',\n",
       "  'blend',\n",
       "  'last',\n",
       "  'operate',\n",
       "  'die',\n",
       "  'exit',\n",
       "  'plump',\n",
       "  'fail',\n",
       "  'endure',\n",
       "  'extend',\n",
       "  'function',\n",
       "  'start',\n",
       "  'break',\n",
       "  'choke',\n",
       "  'lead',\n",
       "  'fit',\n",
       "  'move',\n",
       "  'perish',\n",
       "  'travel',\n",
       "  'locomote',\n",
       "  'get',\n",
       "  'sound',\n",
       "  'run',\n",
       "  'belong',\n",
       "  'rifle',\n",
       "  'live',\n",
       "  'proceed',\n",
       "  'decease',\n",
       "  'croak',\n",
       "  'pass',\n",
       "  'expire',\n",
       "  'become',\n",
       "  'conk',\n",
       "  'work'],\n",
       " 'doctor': ['physician', 'Dr.', 'medico', 'doc', 'MD'],\n",
       " 'next': ['future', 'succeeding', 'following', 'adjacent'],\n",
       " 'monday': ['Mon']}"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default, without filter\n",
    "synonyms_map(sometext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "abe4f84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'need': ['ask', 'take', 'want', 'require'],\n",
       " 'go': ['start', 'break', 'move', 'get', 'run'],\n",
       " 'doctor': ['physician'],\n",
       " 'next': ['future']}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with filter\n",
    "synonyms_map(sometext, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "85b8ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists of most common phrases in questions\n",
    "need = ['do i need to', 'must i', 'is it required that i', 'will i need to']\n",
    "frequency = ['how often do i need', 'what is the timeframe for']\n",
    "scheduling = ['when is my', 'on what date', 'when do i see']\n",
    "insurance = ['is this covered', 'will my insurance cover', 'do i need to pay', 'how much will i pay', \n",
    "             'what is my bill']\n",
    "location = ['where is', 'where can i find', 'how can i find', 'i cant find', 'what is the location', \n",
    "            'can i have the location']\n",
    "ability = ['what can i', 'is there anything i can', 'can i']\n",
    "preparation = ['what do i need', 'how do i prepare', 'how can i get ready for', 'what should i bring']\n",
    "forgetfulness = ['what if i forgot', 'i forgot to', 'is it ok if i forgot']\n",
    "explanation = ['what is', 'tell me what is', 'describe', 'i want to understand']\n",
    "phrasebank = [need, \n",
    "              frequency, \n",
    "              scheduling, \n",
    "              insurance, \n",
    "              location, \n",
    "              ability, \n",
    "              preparation, \n",
    "              forgetfulness, \n",
    "              explanation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "7180d90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['do i need to', 'must i', 'is it required that i', 'will i need to'],\n",
       " ['how often do i need', 'what is the timeframe for'],\n",
       " ['when is my', 'on what date', 'when do i see'],\n",
       " ['is this covered',\n",
       "  'will my insurance cover',\n",
       "  'do i need to pay',\n",
       "  'how much will i pay',\n",
       "  'what is my bill'],\n",
       " ['where is',\n",
       "  'where can i find',\n",
       "  'how can i find',\n",
       "  'i cant find',\n",
       "  'what is the location',\n",
       "  'can i have the location'],\n",
       " ['what can i', 'is there anything i can', 'can i'],\n",
       " ['what do i need',\n",
       "  'how do i prepare',\n",
       "  'how can i get ready for',\n",
       "  'what should i bring'],\n",
       " ['what if i forgot', 'i forgot to', 'is it ok if i forgot'],\n",
       " ['what is', 'tell me what is', 'describe', 'i want to understand']]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrasebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "2d729e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_tokens(utterance, synonyms):\n",
    "    '''return new list of tokens generated from the given utterance using the synonyms.\n",
    "    '''\n",
    "    newutterance = []\n",
    "    \n",
    "    for word in preprocess(utterance).split():\n",
    "        if word in synonyms:\n",
    "            newutterance.append(list(itertools.chain(*[[word], synonyms[word]])))\n",
    "        else:\n",
    "            newutterance.append([word])\n",
    "            \n",
    "    return newutterance\n",
    "\n",
    "def phrase_tokens(utterance, phrases):\n",
    "    '''return new list of tokens generated from the given utterance using the phrases.\n",
    "    '''\n",
    "    newutterances  = []\n",
    "    cleanutterance = preprocess(utterance)\n",
    "    \n",
    "    for phraselist in phrases:\n",
    "        for phrase in phraselist:               \n",
    "            if phrase in cleanutterance:\n",
    "                for i in range(len(phraselist)-1):\n",
    "                    if phraselist[i]==phrase: continue\n",
    "                    copy = cleanutterance\n",
    "                    newutterances.append(copy.replace(phrase, phraselist[i]))\n",
    "                    \n",
    "    return newutterances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2460434a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['will'],\n",
       " ['i'],\n",
       " ['need',\n",
       "  'ask',\n",
       "  'necessitate',\n",
       "  'demand',\n",
       "  'postulate',\n",
       "  'involve',\n",
       "  'take',\n",
       "  'want',\n",
       "  'require'],\n",
       " ['to'],\n",
       " ['go',\n",
       "  'survive',\n",
       "  'depart',\n",
       "  'blend',\n",
       "  'last',\n",
       "  'operate',\n",
       "  'die',\n",
       "  'exit',\n",
       "  'plump',\n",
       "  'fail',\n",
       "  'endure',\n",
       "  'extend',\n",
       "  'function',\n",
       "  'start',\n",
       "  'break',\n",
       "  'choke',\n",
       "  'lead',\n",
       "  'fit',\n",
       "  'move',\n",
       "  'perish',\n",
       "  'travel',\n",
       "  'locomote',\n",
       "  'get',\n",
       "  'sound',\n",
       "  'run',\n",
       "  'belong',\n",
       "  'rifle',\n",
       "  'live',\n",
       "  'proceed',\n",
       "  'decease',\n",
       "  'croak',\n",
       "  'pass',\n",
       "  'expire',\n",
       "  'become',\n",
       "  'conk',\n",
       "  'work'],\n",
       " ['to'],\n",
       " ['the'],\n",
       " ['doctor', 'physician', 'Dr.', 'medico', 'doc', 'MD'],\n",
       " ['again'],\n",
       " ['next', 'future', 'succeeding', 'following', 'adjacent'],\n",
       " ['monday', 'Mon']]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_tokens(sometext, synonyms_map(sometext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "65794105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do i need to go to the doctor again next monday',\n",
       " 'must i go to the doctor again next monday',\n",
       " 'is it required that i go to the doctor again next monday']"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_tokens(sometext, phrasebank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "40356916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_synonyms(utterance, synonyms):\n",
    "    '''utility for generate_utterances() to return a list of generated utterances where\n",
    "    the inputted synonyms from synonym_tokens() are used to replace word in the original\n",
    "    utterance.\n",
    "    '''\n",
    "    genlist = []\n",
    "    clean   = preprocess(utterance)\n",
    "    prev    = 0\n",
    "    \n",
    "    for i in range(len(synonyms)):\n",
    "        slist = synonyms[i]\n",
    "        word = synonyms[i][0]\n",
    "        for j in range(len(slist)):\n",
    "            start = clean.find(word, prev) \n",
    "            end   = start + len(word)\n",
    "            gen   = clean[:start] + slist[j] + clean[end:]\n",
    "            genlist.append(gen)\n",
    "        prev = end  \n",
    "        \n",
    "    return list(set(genlist))\n",
    "\n",
    "def add_phrases(utterance, phrasebank):\n",
    "    '''utility for generate_utterances() to return a list of generated utterances where\n",
    "    the inputted synonyms from synonym_tokens() are used to replace word in the original\n",
    "    utterance.\n",
    "    '''\n",
    "    if not phrasebank: return []\n",
    "    \n",
    "    genlist = []\n",
    "    clean   = preprocess(utterance)\n",
    "    \n",
    "    for plist in phrasebank:\n",
    "        match = [p for p in plist if p in clean]\n",
    "        match = match[0] if match else None\n",
    "        if not match: continue\n",
    "            \n",
    "        start = clean.index(match.split()[0]) \n",
    "        \n",
    "        for p in plist:\n",
    "            end = start + len(match)\n",
    "            gen = clean[:start] + p + clean[end:]\n",
    "            genlist.append(gen)\n",
    "    \n",
    "    return genlist\n",
    "\n",
    "def generate_utterances(utterance, phrasebank=None):\n",
    "    '''return new list of utterances including the inputted utterance and any generated\n",
    "    utterances.\n",
    "    \n",
    "    NOTE  this current method requires validating the generated text manually.\n",
    "    NOTE  synonyms is a list of lists with form [[list of synonym(s)]].\n",
    "    NOTE  phrases is a list with form [list of any phrases].\n",
    "    '''\n",
    "    clean    = preprocess(utterance)\n",
    "    synonyms = synonym_tokens(clean, synonyms_map(clean))\n",
    "    phrases  = phrase_tokens(clean, phrasebank) if phrasebank else []\n",
    "    genlist  = add_phrases(clean, phrasebank)\n",
    "    genlist.extend(add_synonyms(clean, synonyms))\n",
    "    \n",
    "    return genlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e66f8e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do i need to go to the doctor again next monday',\n",
       " 'must i go to the doctor again next monday',\n",
       " 'is it required that i go to the doctor again next monday',\n",
       " 'will i need to go to the doctor again next monday',\n",
       " 'will i need to blend to the doctor again next monday',\n",
       " 'will i need to endure to the doctor again next monday',\n",
       " 'will i need to rifle to the doctor again next monday',\n",
       " 'will i need to die to the doctor again next monday',\n",
       " 'will i take to go to the doctor again next monday',\n",
       " 'will i need to run to the doctor again next monday',\n",
       " 'will i need to belong to the doctor again next monday',\n",
       " 'will i need to go to the doctor again next monday',\n",
       " 'will i involve to go to the doctor again next monday',\n",
       " 'will i want to go to the doctor again next monday',\n",
       " 'will i require to go to the doctor again next monday',\n",
       " 'will i need to function to the doctor again next monday',\n",
       " 'will i need to lead to the doctor again next monday',\n",
       " 'will i need to go to the medico again next monday',\n",
       " 'will i need to croak to the doctor again next monday',\n",
       " 'will i demand to go to the doctor again next monday',\n",
       " 'will i need to break to the doctor again next monday',\n",
       " 'will i need to go to the doctor again adjacent monday',\n",
       " 'will i need to extend to the doctor again next monday',\n",
       " 'will i need to choke to the doctor again next monday',\n",
       " 'will i need to sound to the doctor again next monday',\n",
       " 'will i need to go to the doctor again succeeding monday',\n",
       " 'will i necessitate to go to the doctor again next monday',\n",
       " 'will i need to go to the doctor again following monday',\n",
       " 'will i need to go to the Dr. again next monday',\n",
       " 'will i need to expire to the doctor again next monday',\n",
       " 'will i need to decease to the doctor again next monday',\n",
       " 'will i need to work to the doctor again next monday',\n",
       " 'will i need to conk to the doctor again next monday',\n",
       " 'will i need to perish to the doctor again next monday',\n",
       " 'will i need to fit to the doctor again next monday',\n",
       " 'will i need to travel to the doctor again next monday',\n",
       " 'will i need to go to the MD again next monday',\n",
       " 'will i need to go to the physician again next monday',\n",
       " 'will i postulate to go to the doctor again next monday',\n",
       " 'will i need to proceed to the doctor again next monday',\n",
       " 'will i need to survive to the doctor again next monday',\n",
       " 'will i need to last to the doctor again next monday',\n",
       " 'will i need to exit to the doctor again next monday',\n",
       " 'will i need to fail to the doctor again next monday',\n",
       " 'will i ask to go to the doctor again next monday',\n",
       " 'will i need to operate to the doctor again next monday',\n",
       " 'will i need to live to the doctor again next monday',\n",
       " 'will i need to plump to the doctor again next monday',\n",
       " 'will i need to start to the doctor again next monday',\n",
       " 'will i need to become to the doctor again next monday',\n",
       " 'will i need to move to the doctor again next monday',\n",
       " 'will i need to pass to the doctor again next monday',\n",
       " 'will i need to get to the doctor again next monday',\n",
       " 'will i need to go to the doctor again next Mon',\n",
       " 'will i need to go to the doctor again future monday',\n",
       " 'will i need to go to the doc again next monday',\n",
       " 'will i need to locomote to the doctor again next monday',\n",
       " 'will i need to depart to the doctor again next monday']"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_utterances(sometext, phrasebank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcf75f9",
   "metadata": {},
   "source": [
    "## *Approach 2:* text generation of similar sentences using GANs and/or BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1385a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82b273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c67f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffc281b4",
   "metadata": {},
   "source": [
    "## *Approach 3:* embedding question types and running similarity measure (as alternative if there isn't a direct hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0776aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d16a18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1446a2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
