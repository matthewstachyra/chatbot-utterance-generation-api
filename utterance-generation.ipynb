{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb042e1f",
   "metadata": {},
   "source": [
    "# Utterance generation prototype notebook\n",
    "Author: Matthew Stachyra <br>\n",
    "Date: 16 June 2022 <br>\n",
    "Version: 0.1 - prototyping 3 approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773341b8",
   "metadata": {},
   "source": [
    "## *Approach 1:* replacement with similar words\n",
    "Note: This can be used to generate very similar sentences with similar structure. They may also be used for the machine learning in approaches 2 and 3 below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eb5f34",
   "metadata": {},
   "source": [
    "### Subproblems\n",
    "1. generate possible synonyms\n",
    "2. filter synonyms using similarity measure\n",
    "2. identify which words to replace in an utterance\n",
    "3. replace words in utterance one at a time and generate new set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "25d7d3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/matthewstachyra/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/matthewstachyra/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "063d40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UtteranceGenerator:\n",
    "    '''class to generate utterances from a single utterance.\n",
    "    \n",
    "    USAGE  call generate() to return a list of possible alternatives utterances.\n",
    "    \n",
    "    NOTE  if a model is not input, then synonyms are not filtered by cosine similarity.\n",
    "    '''\n",
    "    def __init__(self, utterance, phrasebank=None, model=None):\n",
    "        self._utterance    = self.preprocess(utterance)\n",
    "        self._posmap       = {'VERB':'v', 'NOUN':'n', 'PRON':'n', 'PROPN':'n', 'ADJ':'a', 'ADV':'r'}\n",
    "        self._phrasebank   = phrasebank\n",
    "        self._model        = model\n",
    "        self._synonymsdict = self.map_synonyms()  # {word : synonyms} for words in utterance\n",
    "\n",
    "        \n",
    "    def preprocess(self, utterance):\n",
    "        '''return list of words in utterance preprocessed to be lower case, removing any non \n",
    "        alphabetic characters, removing words less than 2 characters.\n",
    "        '''\n",
    "        lower     = utterance.lower()\n",
    "        cleanr    = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, '', lower)\n",
    "        rem_num   = re.sub('[0-9]+', '', cleantext)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        return \" \".join(tokenizer.tokenize(rem_num))\n",
    "\n",
    "\n",
    "    def get_pos(self, word):\n",
    "        '''return the part of speech of the word in the utterance if it is a verb\n",
    "        noun, pronoun, proper noun, adjective, or adverb.\n",
    "        '''\n",
    "        if not self._utterance or not word: \n",
    "            raise ValueError(\"Error: Input is empty string.\")\n",
    "            \n",
    "        if self.preprocess(word) not in self._utterance: \n",
    "            raise ValueError(\"Error: The word is not in the utterance.\")\n",
    "\n",
    "        for w in nlp(self._utterance):\n",
    "            if str(w)==word: return w.pos_\n",
    "            \n",
    "            \n",
    "    def get_similarities(self, word, synonyms, model):\n",
    "        '''utility for get_synonyms that returns a dictionary with synonym:cosine similarity key-value pairs.\n",
    "        '''\n",
    "        def cosinesim(v1, v2): \n",
    "            return (np.dot(v1, v2 / (norm(v1) * norm(v2))))\n",
    "        \n",
    "        def embed(vector, model):\n",
    "            try:\n",
    "                vec = model.get_vector(vector)\n",
    "            except:\n",
    "                return np.empty(0)\n",
    "\n",
    "            return vec\n",
    "\n",
    "        sims = {word: 1.0}\n",
    "        ref  = embed(word, model)\n",
    "\n",
    "        for s in synonyms:\n",
    "            vec = embed(s, model)                 \n",
    "            if vec.any():\n",
    "                sim     = cosinesim(ref, vec)\n",
    "                sims[s] = sim\n",
    "\n",
    "        return sims\n",
    "\n",
    "    \n",
    "    def print_similarities(self, similarities):\n",
    "        '''print each word with its cosine similarity to a reference vector.\n",
    "        '''\n",
    "        for synonym, similarity in similarities.items():\n",
    "            print(f\"word: {synonym}, cosine similarity: {similarity}\")\n",
    "\n",
    "            \n",
    "    def get_synonyms(self, word):\n",
    "        '''return synonyms by taking the lemma generated by synsets that have the same part of speech,\n",
    "        given a word if its part of speech is a verb, noun, adverb, or adjective.\n",
    "\n",
    "        NOTE  it is necessary to pass in the pos to get the relevant kind of synonym.\n",
    "        '''\n",
    "        pos = self.get_pos(self.preprocess(word))\n",
    "\n",
    "        if pos not in ['VERB', 'NOUN', 'PRON', 'PROPN', 'ADJ', 'ADV']: return \n",
    "\n",
    "        # get full set of synonyms\n",
    "        synonyms = set(list(itertools.chain([synonym\n",
    "                                             for synset in wn.synsets(word, pos=posmap[pos])\n",
    "                                             for synonym in synset.lemma_names()\n",
    "                                             if len(word)>1])))\n",
    "        \n",
    "        if not self._model: return synonyms\n",
    "        \n",
    "        # filter this set using cosine similarities\n",
    "        similarities = self.get_similarities(word, synonyms, self._model)\n",
    "        \n",
    "        return [synonym \n",
    "                for synonym, similarity in similarities.items() \n",
    "                if similarity>=0.70]\n",
    "    \n",
    "            \n",
    "    def map_synonyms(self):\n",
    "        '''return dictionary of words to synonyms for words, removing any words \n",
    "        that do not have synonyms returned.\n",
    "\n",
    "        NOTE  current version removes ngrams.\n",
    "        '''\n",
    "        d = {}\n",
    "\n",
    "        for word in self._utterance.split():\n",
    "            synonyms = self.get_synonyms(word)\n",
    "            \n",
    "            if synonyms: synonyms = [synonym \n",
    "                                     for synonym in synonyms \n",
    "                                     if len(synonym.split(\"_\"))==1 and self.preprocess(synonym)!=word] \n",
    "                \n",
    "            if synonyms: d[word] = synonyms \n",
    "\n",
    "        return d\n",
    "    \n",
    "\n",
    "    def add_synonyms(self):\n",
    "        '''utility for generate() to return a list of generated utterances where a word's synonyms \n",
    "        are used to replace the word in the original utterance.\n",
    "        '''\n",
    "        genlist = []\n",
    "        tokens  = []\n",
    "        prev    = 0\n",
    "\n",
    "        # generate tokens using synonymsmap\n",
    "        for word in self._utterance.split():\n",
    "            if word in self._synonymsdict:\n",
    "                tokens.append(list(itertools.chain(*[[word], self._synonymsdict[word]])))\n",
    "            else:\n",
    "                tokens.append([word])\n",
    "\n",
    "        # use tokens to return new utterances\n",
    "        for i in range(len(tokens)):\n",
    "            word  = tokens[i][0]\n",
    "            slist = tokens[i]\n",
    "            \n",
    "            for j in range(len(slist)):\n",
    "                start = self._utterance.find(word, prev) \n",
    "                end   = start + len(word)\n",
    "                gen   = self._utterance[:start] + slist[j] + self._utterance[end:]\n",
    "                genlist.append(gen)\n",
    "                \n",
    "            prev = end  \n",
    "\n",
    "        return list(set(genlist))\n",
    "    \n",
    "\n",
    "    def add_phrases(self):\n",
    "        '''utility for generate() to return a list of generated utterances where phrases from the \n",
    "        phrasebank are added if there is any match on at least 1 phrase.\n",
    "        '''\n",
    "        if not self._phrasebank: return []\n",
    "\n",
    "        genlist = []\n",
    "        tokens  = []\n",
    "        \n",
    "        # generate tokens using phrasebank\n",
    "        for plist in self._phrasebank:\n",
    "            for phrase in plist:\n",
    "                if phrase in self._utterance:\n",
    "                    print(\"match\")\n",
    "                    for i in range(len(plist)-1):\n",
    "                        if plist[i]!=phrase:\n",
    "                            copy = self._utterance\n",
    "                            tokens.append(copy.replace(phrase, plist[i]))\n",
    "                            \n",
    "        return tokens\n",
    "    \n",
    "#         # do a recursive call on each token with a new instance of the class\n",
    "#         for token in tokens:\n",
    "#             c = GenerateUtterances(token, model=self._model, phrasebank=self._phrasebank)\n",
    "#             tokens.append(c.generate())\n",
    "        \n",
    "#         return tokens\n",
    "\n",
    "#         # use tokens to return new utterances\n",
    "#         match = [token for token in tokens if token in self._utterance]\n",
    "#         print(match)\n",
    "                \n",
    "#         if match:\n",
    "#             start = self._utterance.index(match[0].split()[0]) \n",
    "\n",
    "#             for token in tokens:\n",
    "#                 end = start + len(match[0])\n",
    "#                 gen = self._utterance[:start] + token + self._utterance[end:]\n",
    "#                 genlist.append(gen)\n",
    "\n",
    "#         return genlist\n",
    "    \n",
    "\n",
    "    def generate(self):\n",
    "        '''return new list of utterances using any synonyms and phrases.\n",
    "\n",
    "        NOTE  this current method requires validating the generated text manually.\n",
    "        \n",
    "        TODO  build django app to provide GUI for selecting which utterances to keep.\n",
    "        '''\n",
    "        generated = self.add_synonyms()\n",
    "        generated.extend(self.add_phrases())\n",
    "\n",
    "        return generated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59cde8",
   "metadata": {},
   "source": [
    "### 1. generate similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204050f",
   "metadata": {},
   "source": [
    "##### using `nltk.corpus.wordnet.synsets` and `spacy` part of speech tagging\n",
    "nltk doc: https://www.nltk.org/howto/wordnet.html <br>\n",
    "spacy doc: https://spacy.io/usage/linguistic-features#pos-tagging <br>\n",
    "pos tags used in spacy: https://universaldependencies.org/u/pos/ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "8dc3f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "need = ['do i need to', 'must i', 'is it required that i', 'will i need to']\n",
    "frequency = ['how often do i need', 'what is the timeframe for']\n",
    "scheduling = ['when is my', 'on what date', 'when do i see']\n",
    "insurance = ['is this covered', 'will my insurance cover', 'do i need to pay', 'how much will i pay', \n",
    "             'what is my bill']\n",
    "location = ['where is', 'where can i find', 'how can i find', 'i cant find', 'what is the location', \n",
    "            'can i have the location']\n",
    "ability = ['what can i', 'is there anything i can', 'can i']\n",
    "preparation = ['what do i need', 'how do i prepare', 'how can i get ready for', 'what should i bring']\n",
    "forgetfulness = ['what if i forgot', 'i forgot to', 'is it ok if i forgot']\n",
    "explanation = ['what is', 'tell me what is', 'describe', 'i want to understand']\n",
    "\n",
    "\n",
    "model      = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "phrasebank = [need, \n",
    "              frequency, \n",
    "              scheduling,\n",
    "              insurance,\n",
    "              location,\n",
    "              ability,\n",
    "              preparation,\n",
    "              forgetfulness,\n",
    "              explanation]\n",
    "utterance  = \"Will I need to go to the doctor again next Monday?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "b85ecf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['will i want to go to the doctor again next monday',\n",
       " 'will i take to go to the doctor again next monday',\n",
       " 'will i need to start to the doctor again next monday',\n",
       " 'will i need to get to the doctor again next monday',\n",
       " 'will i need to go to the doctor again future monday',\n",
       " 'will i need to run to the doctor again next monday',\n",
       " 'will i need to go to the physician again next monday',\n",
       " 'will i need to go to the doctor again next monday',\n",
       " 'will i need to break to the doctor again next monday',\n",
       " 'will i require to go to the doctor again next monday',\n",
       " 'will i ask to go to the doctor again next monday',\n",
       " 'will i need to move to the doctor again next monday',\n",
       " 'do i need to go to the doctor again next monday',\n",
       " 'must i go to the doctor again next monday',\n",
       " 'is it required that i go to the doctor again next monday']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instance of class\n",
    "gu = UtteranceGenerator(utterance, phrasebank=phrasebank, model=model)\n",
    "gu.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2e95fbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will i need to go to the doctor again next monday\n"
     ]
    }
   ],
   "source": [
    "print(gu._utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a72f317d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['need', 'take', 'want', 'require', 'ask']"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gu.get_synonyms('need')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7100a3",
   "metadata": {},
   "source": [
    "### 2. filter synonyms using similarity measure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc902249",
   "metadata": {},
   "source": [
    "##### using `gensim` with `GloVe` embeddings\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f384faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with two models to see which performs best\n",
    "model_glove_twitter = api.load(\"glove-twitter-25\")\n",
    "model_gigaword = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c54b4d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('should', 0.8821427226066589),\n",
       " ('want', 0.8693705797195435),\n",
       " ('we', 0.8659201264381409),\n",
       " ('must', 0.8644395470619202),\n",
       " ('needed', 0.8635740280151367),\n",
       " ('needs', 0.8618939518928528),\n",
       " ('get', 0.8493343591690063),\n",
       " ('make', 0.8488180637359619),\n",
       " ('do', 0.8422726988792419),\n",
       " ('able', 0.8364372849464417)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gigaword.most_similar(positive=['need'],topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5d5c9fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('take', 0.9688727259635925),\n",
       " ('get', 0.9679074883460999),\n",
       " ('give', 0.9652544856071472),\n",
       " ('make', 0.9647879004478455),\n",
       " (\"n't\", 0.9613597393035889),\n",
       " ('better', 0.9595354199409485),\n",
       " (\"'ll\", 0.9594433903694153),\n",
       " ('let', 0.9594159126281738),\n",
       " ('bring', 0.9558347463607788),\n",
       " ('have', 0.9553070068359375)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove_twitter.most_similar(positive=['need'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9523a0",
   "metadata": {},
   "source": [
    "**Note:** it seems that the wikipedia model provides more relevant 'synonyms' than the model trained on twitter. The below code works exclusively with `model_gigaword`, or the embeddings trained on wikipedia data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4218526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(vector, model):\n",
    "    '''return a (100,) embedding of the vector using the model.\n",
    "    '''\n",
    "    try:\n",
    "        vec = model.get_vector(vector)\n",
    "    except:\n",
    "        return np.empty(0)\n",
    "    \n",
    "    return vec\n",
    "\n",
    "    \n",
    "def get_similarities(word, synonyms, model):\n",
    "    '''return dictionary with synonym:cosine similarity key-value pairs.\n",
    "    '''\n",
    "    cosinesim = lambda v1, v2: (np.dot(v1, v2 / (norm(v1) * norm(v2))))\n",
    "    \n",
    "    sims = {word: 1.0}\n",
    "    ref  = embed(word, model)\n",
    "    \n",
    "    for s in synonyms:\n",
    "        vec = embed(s, model)                 \n",
    "        if vec.any():\n",
    "            sim     = cosinesim(ref, vec)\n",
    "            sims[s] = sim\n",
    "    \n",
    "    return sims\n",
    "\n",
    "        \n",
    "def filter_synonyms(similarities, threshold=0.70):\n",
    "    '''return subset of dictionary where the similarity is at least the threshold\n",
    "    value, with a default of 0.70 cosine similarity.\n",
    "    '''\n",
    "    return [synonym \n",
    "            for synonym, similarity in similarities.items() \n",
    "            if similarity>=threshold]\n",
    "    \n",
    "\n",
    "def print_similarities(similarities):\n",
    "    '''print each word with its cosine similarity to a reference vector.\n",
    "    '''\n",
    "    for synonym, similarity in similarities.items():\n",
    "        print(f\"word: {synonym}, cosine similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e632f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.043244, -0.47529 ,  0.15808 ,  0.20413 , -0.15383 ,  0.72284 ,\n",
       "        0.26145 ,  0.20892 , -0.3147  , -0.070307, -0.43367 ,  0.053109,\n",
       "        0.73635 ,  0.98111 ,  0.23535 , -0.10449 ,  0.50258 , -0.033356,\n",
       "       -0.35537 ,  0.64549 , -0.37103 , -0.10052 , -0.76929 , -0.16957 ,\n",
       "       -0.15648 ,  0.53548 ,  0.35146 , -1.5126  ,  0.050984,  0.24445 ,\n",
       "       -0.35688 ,  0.43968 , -0.62985 ,  0.32891 , -0.53009 ,  0.49832 ,\n",
       "       -1.2061  ,  0.27797 ,  0.42734 ,  0.095773, -0.43527 ,  0.93561 ,\n",
       "        0.36039 , -0.83114 ,  0.12966 , -0.1363  , -0.58124 ,  0.092946,\n",
       "       -0.014708,  0.32562 ,  0.41204 ,  0.1451  ,  0.49803 ,  0.86926 ,\n",
       "       -0.18033 , -1.6227  , -0.64565 ,  0.17504 ,  0.73849 ,  0.39156 ,\n",
       "        0.83135 ,  0.51308 ,  0.12999 , -0.21288 ,  0.68456 ,  0.056297,\n",
       "        0.090792,  0.28032 , -0.12233 ,  0.60761 , -0.57913 , -0.024127,\n",
       "       -0.063252,  0.40747 ,  0.10775 ,  0.57977 ,  0.092789, -0.15588 ,\n",
       "       -0.36494 , -0.46632 ,  0.37553 ,  0.164   , -0.75138 , -0.10833 ,\n",
       "       -2.1363  , -0.11658 ,  0.23157 ,  0.24425 , -0.36352 , -0.34928 ,\n",
       "        0.60001 , -0.10529 ,  0.78614 ,  0.45707 , -0.1712  ,  0.6362  ,\n",
       "        0.68622 , -0.28397 , -0.29773 , -0.81148 ], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gigaword.get_vector('doctor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a85d9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: need, cosine similarity: 1.0\n",
      "word: take, cosine similarity: 0.8283353447914124\n",
      "word: postulate, cosine similarity: 0.018827084451913834\n",
      "word: necessitate, cosine similarity: 0.18295712769031525\n",
      "word: involve, cosine similarity: 0.5790201425552368\n",
      "word: want, cosine similarity: 0.8693705797195435\n",
      "word: require, cosine similarity: 0.7587778568267822\n",
      "word: demand, cosine similarity: 0.6003023386001587\n",
      "word: ask, cosine similarity: 0.7399007678031921\n"
     ]
    }
   ],
   "source": [
    "word = 'need' \n",
    "syns = gen.get_synonyms(word)\n",
    "sims = gen.get_similarities(word, syns, model_gigaword)\n",
    "gen.print_similarities(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "faac6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context for the word is: \n",
      " 'will i need to go to the doctor again next monday'. \n",
      "\n",
      "Full list of synonyms is: \n",
      " ['need', 'take', 'postulate', 'call_for', 'necessitate', 'involve', 'want', 'require', 'demand', 'ask']. \n",
      "\n",
      "Filtered list of synonyms is: \n",
      " ['need', 'take', 'want', 'require', 'ask'].\n"
     ]
    }
   ],
   "source": [
    "print(f\"The context for the word is: \\n '{clean}'. \\n\")\n",
    "print(f\"Full list of synonyms is: \\n {list(gen.get_synonyms(word))}. \\n\")\n",
    "print(f\"Filtered list of synonyms is: \\n {gen.filter_synonyms(sims)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec420d71",
   "metadata": {},
   "source": [
    "### 3. select which words to replace \n",
    "Note: only replacing nouns, verbs, pronouns, proper nouns, adjectives, and adverbs <br>\n",
    "spacy doc: https://spacy.io/usage/linguistic-features#pos-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6742463b",
   "metadata": {},
   "source": [
    "##### using `spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b6e6158a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUX\n",
      "PRON\n",
      "VERB\n",
      "PART\n",
      "VERB\n",
      "ADP\n",
      "DET\n",
      "NOUN\n",
      "ADV\n",
      "ADP\n",
      "PROPN\n",
      "PUNCT\n"
     ]
    }
   ],
   "source": [
    "for word in nlp(sometext):\n",
    "    print(word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "49283613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VERB'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gu.get_pos('need')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d28f3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_synonyms(utterance, synonymfilter=False):\n",
    "    '''return dictionary of words to synonyms for words, removing any words \n",
    "    that do not have synonyms returned.\n",
    "    \n",
    "    NOTE  current version removes ngrams.\n",
    "    '''\n",
    "    d = {}\n",
    "    \n",
    "    for word in preprocess(utterance).split():\n",
    "        synonyms = get_synonyms(word, clean)\n",
    "        if synonyms:\n",
    "            similarities = get_similarities(word, synonyms, model_gigaword)\n",
    "            synonyms     = filter_synonyms(similarities) if synonymfilter else synonyms\n",
    "            synonyms     = [synonym\n",
    "                            for synonym in synonyms \n",
    "                            if len(synonym.split(\"_\"))==1 and preprocess(synonym)!=word] \n",
    "        \n",
    "        if synonyms: d[word] = synonyms \n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0317fd3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'need': ['take', 'want', 'require', 'ask'],\n",
       " 'go': ['move', 'run', 'start', 'break', 'get'],\n",
       " 'doctor': ['physician'],\n",
       " 'next': ['future']}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gu.map_synonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "85b8ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of lists of most common phrases in questions\n",
    "need = ['do i need to', 'must i', 'is it required that i', 'will i need to']\n",
    "frequency = ['how often do i need', 'what is the timeframe for']\n",
    "scheduling = ['when is my', 'on what date', 'when do i see']\n",
    "insurance = ['is this covered', 'will my insurance cover', 'do i need to pay', 'how much will i pay', \n",
    "             'what is my bill']\n",
    "location = ['where is', 'where can i find', 'how can i find', 'i cant find', 'what is the location', \n",
    "            'can i have the location']\n",
    "ability = ['what can i', 'is there anything i can', 'can i']\n",
    "preparation = ['what do i need', 'how do i prepare', 'how can i get ready for', 'what should i bring']\n",
    "forgetfulness = ['what if i forgot', 'i forgot to', 'is it ok if i forgot']\n",
    "explanation = ['what is', 'tell me what is', 'describe', 'i want to understand']\n",
    "phrasebank = [need, \n",
    "              frequency, \n",
    "              scheduling, \n",
    "              insurance, \n",
    "              location, \n",
    "              ability, \n",
    "              preparation, \n",
    "              forgetfulness, \n",
    "              explanation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7180d90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['do i need to', 'must i', 'is it required that i', 'will i need to'],\n",
       " ['how often do i need', 'what is the timeframe for'],\n",
       " ['when is my', 'on what date', 'when do i see'],\n",
       " ['is this covered',\n",
       "  'will my insurance cover',\n",
       "  'do i need to pay',\n",
       "  'how much will i pay',\n",
       "  'what is my bill'],\n",
       " ['where is',\n",
       "  'where can i find',\n",
       "  'how can i find',\n",
       "  'i cant find',\n",
       "  'what is the location',\n",
       "  'can i have the location'],\n",
       " ['what can i', 'is there anything i can', 'can i'],\n",
       " ['what do i need',\n",
       "  'how do i prepare',\n",
       "  'how can i get ready for',\n",
       "  'what should i bring'],\n",
       " ['what if i forgot', 'i forgot to', 'is it ok if i forgot'],\n",
       " ['what is', 'tell me what is', 'describe', 'i want to understand']]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrasebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d729e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_tokens(utterance, synonyms):\n",
    "    '''return new list of tokens generated from the given utterance using the synonyms.\n",
    "    '''\n",
    "    newutterance = []\n",
    "    \n",
    "    for word in preprocess(utterance).split():\n",
    "        if word in synonyms:\n",
    "            newutterance.append(list(itertools.chain(*[[word], synonyms[word]])))\n",
    "        else:\n",
    "            newutterance.append([word])\n",
    "            \n",
    "    return newutterance\n",
    "\n",
    "def phrase_tokens(utterance, phrases):\n",
    "    '''return new list of tokens generated from the given utterance using the phrases.\n",
    "    '''\n",
    "    newutterances  = []\n",
    "    cleanutterance = preprocess(utterance)\n",
    "    \n",
    "    for phraselist in phrases:\n",
    "        for phrase in phraselist:               \n",
    "            if phrase in cleanutterance:\n",
    "                for i in range(len(phraselist)-1):\n",
    "                    if phraselist[i]==phrase: continue\n",
    "                    copy = cleanutterance\n",
    "                    newutterances.append(copy.replace(phrase, phraselist[i]))\n",
    "                    \n",
    "    return newutterances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2460434a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['will'],\n",
       " ['i'],\n",
       " ['need', 'take', 'want', 'require', 'ask'],\n",
       " ['to'],\n",
       " ['go', 'move', 'run', 'start', 'break', 'get'],\n",
       " ['to'],\n",
       " ['the'],\n",
       " ['doctor', 'physician'],\n",
       " ['again'],\n",
       " ['next', 'future'],\n",
       " ['monday']]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gu.synonym_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "65794105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do i need to go to the doctor again next monday',\n",
       " 'must i go to the doctor again next monday',\n",
       " 'is it required that i go to the doctor again next monday']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gu.phrase_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40356916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_synonyms(utterance, synonyms):\n",
    "    '''utility for generate_utterances() to return a list of generated utterances where\n",
    "    the inputted synonyms from synonym_tokens() are used to replace word in the original\n",
    "    utterance.\n",
    "    '''\n",
    "    genlist = []\n",
    "    clean   = preprocess(utterance)\n",
    "    prev    = 0\n",
    "    \n",
    "    for i in range(len(synonyms)):\n",
    "        slist = synonyms[i]\n",
    "        word = synonyms[i][0]\n",
    "        for j in range(len(slist)):\n",
    "            start = clean.find(word, prev) \n",
    "            end   = start + len(word)\n",
    "            gen   = clean[:start] + slist[j] + clean[end:]\n",
    "            genlist.append(gen)\n",
    "        prev = end  \n",
    "        \n",
    "    return list(set(genlist))\n",
    "\n",
    "def add_phrases(utterance, phrasebank):\n",
    "    '''utility for generate_utterances() to return a list of generated utterances where\n",
    "    the inputted synonyms from synonym_tokens() are used to replace word in the original\n",
    "    utterance.\n",
    "    '''\n",
    "    if not phrasebank: return []\n",
    "    \n",
    "    genlist = []\n",
    "    clean   = preprocess(utterance)\n",
    "    \n",
    "    for plist in phrasebank:\n",
    "        match = [p for p in plist if p in clean]\n",
    "        match = match[0] if match else None\n",
    "        if not match: continue\n",
    "            \n",
    "        start = clean.index(match.split()[0]) \n",
    "        \n",
    "        for p in plist:\n",
    "            end = start + len(match)\n",
    "            gen = clean[:start] + p + clean[end:]\n",
    "            genlist.append(gen)\n",
    "    \n",
    "    return genlist\n",
    "\n",
    "def generate_utterances(utterance, phrasebank=None):\n",
    "    '''return new list of utterances including the inputted utterance and any generated\n",
    "    utterances.\n",
    "    \n",
    "    NOTE  this current method requires validating the generated text manually.\n",
    "    NOTE  synonyms is a list of lists with form [[list of synonym(s)]].\n",
    "    NOTE  phrases is a list with form [list of any phrases].\n",
    "    '''\n",
    "    clean    = preprocess(utterance)\n",
    "    synonyms = synonym_tokens(clean, synonyms_map(clean))\n",
    "    phrases  = phrase_tokens(clean, phrasebank) if phrasebank else []\n",
    "    genlist  = add_phrases(clean, phrasebank)\n",
    "    genlist.extend(add_synonyms(clean, synonyms))\n",
    "    \n",
    "    return genlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e66f8e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match\n",
      "['do i need to go to the doctor again next monday', 'must i go to the doctor again next monday', 'is it required that i go to the doctor again next monday']\n"
     ]
    }
   ],
   "source": [
    "gu.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f32cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bcf75f9",
   "metadata": {},
   "source": [
    "## *Approach 2:* text generation of similar sentences using GANs and/or BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1385a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82b273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c67f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffc281b4",
   "metadata": {},
   "source": [
    "## *Approach 3:* embedding question types and running similarity measure (as alternative if there isn't a direct hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0776aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d16a18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1446a2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
